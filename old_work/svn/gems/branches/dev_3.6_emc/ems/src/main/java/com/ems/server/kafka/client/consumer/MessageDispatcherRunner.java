/**
 * 
 */
package com.ems.server.kafka.client.consumer;

import java.util.HashMap;
import java.util.Iterator;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.LinkedBlockingQueue;
import java.util.concurrent.PriorityBlockingQueue;
import java.util.concurrent.ThreadPoolExecutor;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicBoolean;

import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.log4j.Logger;

import com.ems.server.device.DeviceListener;
import com.ems.server.device.ZigbeeDeviceImpl;
import com.ems.server.util.ServerUtil;

/**
 * The <code>MessageDispatcherRunner</code> class provides {@link Runnable} workers that are used to service the
 * PriorityBlockingQueue of incoming kafka messages. It publishes the messages on the websocket associated with the 
 * gateway associated with the KafkaMessage
 * @author Shyam
 *
 */
public class MessageDispatcherRunner implements Runnable {

	/***
	 * The PriorityBlockingQueue that holds all incoming messages from all KafkaConsumers
	 */
	private final PriorityBlockingQueue<ConsumerRecords<String, byte[]>> queue;
	/***
	 * Threadsafe boolean to track whether this worker is getting closed
	 */
	private final AtomicBoolean closed = new AtomicBoolean(false);
	/***
	 * Logger
	 */
	//private static final Logger logger = LogManager.getFormatterLogger(MessageDispatcherRunner.class);
	private static Logger logger = Logger.getLogger("KafkaLogger");

	//this map is to hold thread pools for each gateway
  private ConcurrentHashMap<String, GwThreadPoolExecutor> gwThreadPoolMap = 
    new ConcurrentHashMap<String, GwThreadPoolExecutor>(); 
  
	/****
	 * Instantiates a <code>MessageDispatcherRunner</code> object with the PriorityQueue containing the inbound kafka messages
	 * @param queue
	 */
	public MessageDispatcherRunner(PriorityBlockingQueue<ConsumerRecords<String, byte[]>> queue) {
		this.queue = queue;
	}

	/***
	 * {@inheritDoc}
	 */
	@Override
	public void run() {
		while (!closed.get()) {
			try {
				Thread.sleep(10); //Since there is no IO on this thread we need to sleep or it will kill the CPU
				ConsumerRecords<String, byte[]> consumerRecords = queue.take();
				Iterator<ConsumerRecord<String, byte[]>> conRecIter = consumerRecords.iterator();
				while(conRecIter.hasNext()) {
					ConsumerRecord<String, byte[]> consumerRecord = conRecIter.next();
					if(logger.isDebugEnabled()) {
						logger.info("Message received from GW " + consumerRecord.topic() + " " 
								+ ServerUtil.getLogPacket(consumerRecord.value()) + " " + consumerRecord.offset()	);
					}
					String gwMac = extractGatewayMac(consumerRecord.topic());
					if(consumerRecord.value()[0] == 0x7b) {
						//this is a message generated by kafka proxy once a web socket connect is established/closed.
						//TODO this is to be handled by string consumer. for some reason it is not being consumed by string consumer but
						//being consumed by binary consumer itself.
						//KafkaProxyEndpoint.getInstance().toggleGatewayReachability(gwMac);
						continue;
					}
					//ZigbeeDeviceImpl.getInstance().getDeviceListener().addGatewayKafkaResponse(consumerRecord.value(), gwMac);
					GwThreadPoolExecutor gwThrPool = gwThreadPoolMap.get(gwMac);
					if(gwThrPool == null) {
						LinkedBlockingQueue<Runnable> workQueue = new LinkedBlockingQueue<Runnable>(); 
						gwThrPool = new GwThreadPoolExecutor(1, 1, 0, TimeUnit.MILLISECONDS, workQueue);
						gwThreadPoolMap.put(gwMac, gwThrPool);
						
					}
					KafkaGwJob job = new KafkaGwJob(gwMac, consumerRecord.value());
					gwThrPool.execute(job);
				}
			} catch (InterruptedException e) {
				logger.error(e);
			}
		}
	}
	
	public class KafkaGwJob implements Runnable {
    
		private String gwMac = null;
    private byte[] data = null;  

    public KafkaGwJob(String gwMac, byte[] data) {
    	
    	this.gwMac = gwMac;
    	this.data = data;
    	
    }
    
    public void run() {

    	GwMessageProcessor gwProcessor = gwProcessorMap.get(gwMac);
    	if(gwProcessor == null) {
    		gwProcessor = new GwMessageProcessor(gwMac);
    		gwProcessorMap.put(gwMac, gwProcessor);
    	}
    	gwProcessor.processData(data);
    	
    } //end of method run
    
	} //end of class KafkaGwJob 
	
	public class GwThreadPoolExecutor extends ThreadPoolExecutor {
  
		public GwThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, 
				TimeUnit unit, LinkedBlockingQueue workQueue) { 
    
			super(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue);
			prestartAllCoreThreads();
			allowCoreThreadTimeOut(false);
    
		} //end of constructor

		protected void beforeExecute(Thread t, Runnable r) {
			
			super.beforeExecute(t, r);      
    
		} //end of method beforeExecute
  
		public void execute(KafkaGwJob job) {
    
			super.execute(job); 
    
		} //end of method execute
  
		protected void afterExecute(Runnable r, Throwable t) {
    
			super.afterExecute(r, t);
    
		} //end of method afterExecute
  
	} //end of class GwThreadPoolExecutor 

	private static HashMap<String, GwMessageProcessor> gwProcessorMap = new HashMap<String, GwMessageProcessor>();
	
	public class GwMessageProcessor {
		
		private String gwMac;
		private int rcvdHeaderLen = 0;
		private int expLen = 0;
		private int currLen = 0;
		private byte[] rcvdPkt = null;
		private byte firstLen = 0;
		private boolean poolPkt = true;
		
		public GwMessageProcessor(String gwMac) {
			this.gwMac = gwMac;
		}
		
		public void processData(byte[] data) {

			int len = data.length;
//			if(logger.isDebugEnabled()) {
//				logger.debug(gwMac + " current len - " + currLen);
//				logger.debug(gwMac + " len -- " + len);
//				logger.debug(gwMac + " rcvd header len  -- " + rcvdHeaderLen);
//				logger.debug(gwMac + " exp len - " + expLen);
//			}
			
			int currBuffPos = 0;
			
			for (; len > 0;) {
				if (rcvdHeaderLen == 4) {
					// header is received
					int left = expLen - currLen;
					int toCopy = len > left ? left : len;
					System.arraycopy(data, currBuffPos, rcvdPkt, currLen + 4,
							toCopy);
					len -= toCopy;
					currBuffPos += toCopy;
					currLen += toCopy;
					/*
					if (logger.isDebugEnabled()) {
						logger.debug(gwMac + " no. of bytes to copy - " + left);
						logger.debug(gwMac + " to copy -- " + toCopy);
						logger.debug(gwMac + " current len -- " + currLen);
					}
					*/
					if (currLen == expLen) {
						// got the full packet, send for processing it
						if (logger.isInfoEnabled()) {
							logger.info(gwMac + ": received command - " + ServerUtil.getLogPacket(rcvdPkt));
						}
//						if (logger.isDebugEnabled()) {
//							logger.debug(gwMac + ": received " + expLen + " bytes");
//						}
						ZigbeeDeviceImpl.getInstance().getDeviceListener().addGatewayKafkaResponse(rcvdPkt, gwMac, poolPkt);
						rcvdHeaderLen = 0;
						expLen = 0;
						currLen = 0;
					}
				} else {
					// we don't have the header yet
					byte c = data[currBuffPos++];
					len--;
					switch (rcvdHeaderLen) {
					case 0:
						// first byte e
						if (c == 0x65) {
							rcvdHeaderLen = 1;
						}
						break;
					case 1:
						// second byte s
						if (c == 0x73) {
							rcvdHeaderLen = 2;
						} else {
							rcvdHeaderLen = 0;
						}
						break;
					case 2:
						// first byte of len
						firstLen = c;
						expLen = c << 8;
						rcvdHeaderLen = 3;
						break;
					case 3:
						// second byte of len
						expLen |= (c & 0xFF) - 4;
//						if (logger.isDebugEnabled()) {
//							logger.debug(gwMac + " expected length of the packet -- " + expLen);
//						}
						rcvdHeaderLen = 4;
						currLen = 0;
						poolPkt = true;
						if (expLen > 128) {
							// pool pkts are of size 128. So, create a temporary
							// packet for length of
							// more than 128
							rcvdPkt = new byte[expLen + 4];
							poolPkt = false;
						} else {
							rcvdPkt = DeviceListener.getPacket();
							if (rcvdPkt == null) {
								// pool is exhausted. so create a temporary packet
								// which will be garbage collected
								rcvdPkt = new byte[expLen + 4];
								poolPkt = false;
							}
						}
						rcvdPkt[0] = 0x65;
						rcvdPkt[1] = 0x73;
						rcvdPkt[2] = firstLen;
						rcvdPkt[3] = c;
						break;
					}
				}
			}

		} // end of method processData
		
	} //end of class GwProcessor

	private String extractGatewayMac(String topic) {
		
		String gwId = extractGatewayId(topic);
		String gwMac = gwId.replaceAll("..(?!$)",  "$0:");
		return gwMac;
		
	}

	/***
	 * Extract the gatewayId from the topic
	 * @param string representing the topic
	 * @return the extracted gatewayId
	 */
	private String extractGatewayId(String topic) {
		return topic.split("-")[0];
	}

	/***
	 * Shuts down this worker
	 */
	public void shutdown() {
		closed.set(true);
	}

}
